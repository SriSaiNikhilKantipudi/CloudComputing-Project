# -*- coding: utf-8 -*-
"""CC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q_PsAl-2fMfFGqKULPFNkSOKYuzxOv5m
"""

!pip3 install twint

import twint



c = twint.Config()
c.Search = "Crime OR Criminal OR Vehicular OR Warrant OR Arrest OR Stealing OR Shoplift OR Damage OR Arson OR Embezzlement OR Suicide OR Domestic Violence"
c.Since = "2018-12-31"
c.Until = "2019-12-31"
c.Near = "Kansas City"
c.Limit = 1
c.Pandas = True
twint.run.Search(c)
df = twint.storage.panda.Tweets_df

import twint
tlist = ["Crime","Criminal","Vehicular","Warrant","Arrest","Stealing","Shoplift","Damage","Arson","Embezzlement","Suicide","Assault","Assault","Sodomy","Rape","Statutory","Violence","RobberyArmed","Forgery","Casualty","Burglary","Stolen","Rape","Prostitution","Robbery","Embezzlement","Molestation","Casualty","Abuse","Suicide","Terroristic","Theft","Interdiction","terrorist"]
for t in tlist:
  c = twint.Config()
  c.Search = t
  c.Since = "2018-12-31"
  c.Until = "2019-12-31"
  c.Near = "Kansas City"
  c.Limit = 2000
  c.Pandas = True
  twint.run.Search(c)
  dff = twint.storage.panda.Tweets_df
  print(dff)
  df = df.append(dff)

df

import nltk
nltk.download('stopwords')

from textblob import TextBlob
import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords

df['sentiment'] = df['tweet'].apply(lambda tweet: TextBlob(tweet).sentiment)

df['polarity_score'] = df['tweet'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)

df['polarity'] = df['polarity_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))

df['polarity'].value_counts()

df[(df.polarity=='negative')]

def clean_text(text):
    tweet = re.sub("(@_?[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",text).split() # removes special characters
    filtered_words = [word for word in tweet if word not in stopwords.words('english')] # acquires non-stopwords words
    return filtered_words

df['tweet_clean'] = df['tweet'].map(lambda x: clean_text(str(x)))

result = df[['date','username','near','hashtags','tweet','tweet_clean','polarity']]

df[['tweet_clean']].to_csv('spark.csv', encoding='utf-8')

result.to_csv('result.csv', encoding='utf-8')